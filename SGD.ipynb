{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class logistic_regression():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lr_rate=0.001, \n",
    "                 alpha=0.001, \n",
    "                 n_iter_no_change=5, \n",
    "                 max_iter=1000, \n",
    "                 tol=1e-9):\n",
    "        \n",
    "        self.lr_rate = lr_rate # eta (constant that multiplies the weight update term in update function)\n",
    "        self.alpha = alpha #lambda (constant that multiplies the regularization term in optimization function)\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self._early_stoing = False\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-(x@self.w+self.b)))\n",
    "        \n",
    "    def log_loss(self, x, y):\n",
    "        log_loss, infi = 0, 1e-90\n",
    "        for x1, y1 in zip(x, y):\n",
    "            prob_pos_class = self.sigmoid(x1)\n",
    "            # adding 1e-90 to avoid log(0) math error\n",
    "            log_loss += y1*np.log(prob_pos_class+infi) + (1-y1)*np.log(1-prob_pos_class+infi)\n",
    "        return -log_loss/x.shape[0]\n",
    "        \n",
    "    def initializations(self, \n",
    "                        x_tr, \n",
    "                        y_tr, \n",
    "                        x_cv, \n",
    "                        y_cv):\n",
    "        self.wait = 0\n",
    "        self.n = x_tr.shape[0]\n",
    "        self.x_tr, self.y_tr = x_tr, y_tr\n",
    "        self.x_cv, self.y_cv = x_cv, y_cv\n",
    "        self.w, self.b = self.np.zeros_like(x_tr[0]), 0\n",
    "        self.tr_log_loss_lst = [self.log_loss(self.x_tr, self.y_tr)]\n",
    "        self.cv_log_loss_lst = [self.log_loss(self.x_cv, self.y_cv)]\n",
    "        \n",
    "    def evaluate_the_model(self, epoch):\n",
    "        self.tr_log_loss_lst.append(self.log_loss(self.x_tr, self.y_tr))\n",
    "        self.cv_log_loss_lst.append(self.log_loss(self.x_cv, self.y_cv))\n",
    "        print(f\"---> Epoch {epoch}\")\n",
    "        print(f\"Tr ave loss {self.tr_log_loss_lst[-1]} & CV ave loss {self.cv_log_loss_lst[-1]}.\")\n",
    "        print(f\"Total training time: {np.round(time.time()-self.now, 3)} seconds.\")\n",
    "        \n",
    "    def early_stopping(self, epoch):\n",
    "        if (self.tr_log_loss_lst[-2] - self.tr_log_loss_lst[-1]) <= self.tol:\n",
    "            if self.wait == self.n_iter_no_change:\n",
    "                temp = np.round(time.time()-self.now, 3)\n",
    "                print(f\"Convergence after {epoch} epochs time took: {temp} seconds.\")\n",
    "                self._early_stoing = True\n",
    "            self.wait += 1\n",
    "        else:\n",
    "            self.wait = 0\n",
    "        \n",
    "    def fit(self, \n",
    "            x_tr, \n",
    "            y_tr, \n",
    "            x_cv, \n",
    "            y_cv):\n",
    "        \n",
    "        self.initializations(x_tr, \n",
    "                             y_tr, \n",
    "                             x_cv, \n",
    "                             y_cv)\n",
    "        self.now = time.time()\n",
    "        for epoch in range(1, self.max_iter+1):\n",
    "            if self._early_stoing == False:\n",
    "                for x, y in zip(self.x_tr, self.y_tr):\n",
    "                    common = self.alpha*(y-self.sigmoid(x))\n",
    "                    self.w = (1-self.lr_rate*self.alpha/self.n)*self.w + common*x\n",
    "                    self.b += common\n",
    "                self.evaluate_the_model(epoch)\n",
    "                self.early_stopping(epoch)\n",
    "            else: break\n",
    "    \n",
    "    def predict_class(self, x):\n",
    "        return np.array([1 if (self.sigmoid(x1)>0.5) else 0 for x1 in x])\n",
    "    \n",
    "    def plot_log_loss_per_epoch(self, ):\n",
    "        plt.figure(figsize=(18, 8))\n",
    "        plt.suptitle('Ploting the log_loss VS Epochs with our Custom SGD Results', fontsize=30)\n",
    "        \n",
    "        plt.plot(range(1, len(self.tr_log_loss_lst)+1), \n",
    "                 self.tr_log_loss_lst, \n",
    "                 marker='o', \n",
    "                 label='Change in the tr_log_loss per each epoch')\n",
    "        plt.plot(range(1, len(self.cv_log_loss_lst)+1), \n",
    "                 self.cv_log_loss_lst, \n",
    "                 marker='o', \n",
    "                 label='Change in the te_log_loss per each epoch')\n",
    "        \n",
    "        plt.xticks(range(1, len(self.tr_log_loss_lst)+1), fontsize=15)\n",
    "        plt.yticks(fontsize=15)\n",
    "        plt.xlabel('Number of Epochs', fontsize=15)\n",
    "        plt.ylabel('log_loss Value', fontsize=15)\n",
    "        plt.legend(fontsize=15)\n",
    "        \n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=50000, \n",
    "                           n_features=15, \n",
    "                           n_informative=10, \n",
    "                           n_redundant=5,\n",
    "                           n_classes=2, \n",
    "                           weights=[0.7], \n",
    "                           class_sep=0.7, \n",
    "                           random_state=15)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'logistic_regression' object has no attribute 'np'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m logistic_regression()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, X_test, y_test)\n",
      "\u001b[1;32m/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         x_tr, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         y_tr, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m         x_cv, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m         y_cv):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitializations(x_tr, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m                          y_tr, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m                          x_cv, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m                          y_cv)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnow \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n",
      "\u001b[1;32m/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_tr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_tr \u001b[39m=\u001b[39m x_tr, y_tr\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_cv, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_cv \u001b[39m=\u001b[39m x_cv, y_cv\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnp\u001b[39m.\u001b[39mzeros_like(x_tr[\u001b[39m0\u001b[39m]), \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtr_log_loss_lst \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_loss(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_tr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_tr)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv_log_loss_lst \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_loss(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_cv, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_cv)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'logistic_regression' object has no attribute 'np'"
     ]
    }
   ],
   "source": [
    "model = logistic_regression()\n",
    "model.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
