{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semi supervised learning\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class SelfLearningModel(BaseEstimator):\n",
    "    def __init__(self, basemodel, max_iter=200, prob_threshold=0.8):\n",
    "        self.model = basemodel\n",
    "        self.max_iter = max_iter\n",
    "        self.prob_threshold = prob_threshold\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        labeled_mask = y != -1\n",
    "        labeled_X, labeled_y = X[labeled_mask], y[labeled_mask]\n",
    "        unlabeled_X = X[~labeled_mask]\n",
    "        self.model.fit(labeled_X, labeled_y)\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            unlabeled_y = self.model.predict(unlabeled_X)\n",
    "            unlabeled_prob = np.max(self.model.predict_proba(unlabeled_X), axis=1)\n",
    "            confident_indices = unlabeled_prob > self.prob_threshold\n",
    "            if not np.any(confident_indices):\n",
    "                break\n",
    "            labeled_X = np.vstack([labeled_X, unlabeled_X[confident_indices]])\n",
    "            labeled_y = np.hstack([labeled_y, unlabeled_y[confident_indices]])\n",
    "            unlabeled_X = unlabeled_X[~confident_indices]\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9666666666666667\n",
      "Random Forest Accuracy: 0.9833333333333333\n",
      "Extra Trees Accuracy: 0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree, Random Forest, and Extremely Randomized Trees\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Initialize the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "et_classifier = ExtraTreesClassifier(n_estimators=90, random_state=42)\n",
    "\n",
    "# Train the classifiers\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "et_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "dt_pred = dt_classifier.predict(X_test)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "et_pred = et_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "et_accuracy = accuracy_score(y_test, et_pred)\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Extra Trees Accuracy:\", et_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohansridhar/miniforge3/envs/google/lib/python3.10/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# adaboost \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the base estimator (weak learner)\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Initialize the AdaBoost classifier with DecisionTreeClassifier as the base estimator\n",
    "ab_classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "ab_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "ab_pred = ab_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "ab_accuracy = accuracy_score(y_test, ab_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"AdaBoost Accuracy:\", ab_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @Flip how are you not ded\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "text = \"\"\"\\xa0@Flip\\xa0how are you not ded\"\"\"\n",
    "\n",
    "# Use BeautifulSoup to handle the special characters\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "clean_text = soup.get_text()\n",
    "\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send your feedback to _EM or contact us at _EM\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_emails(text):\n",
    "    email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    return re.sub(email_regex, '_EM', text)\n",
    "\n",
    "text = \"Send your feedback to example@email.com or contact us at info@company.com\"\n",
    "cleaned_text = replace_emails(text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This',), ('is',), ('a',), ('sample',), ('text',), ('for',), ('generating',), ('unigrams',), ('.',)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a sample text for generating unigrams.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Generate unigrams\n",
    "unigrams = list(ngrams(words, 1))\n",
    "\n",
    "print(unigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'text'), ('text', 'for'), ('for', 'generating'), ('generating', 'bigrams'), ('bigrams', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a sample text for generating bigrams.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams = list(ngrams(words, 2))\n",
    "\n",
    "print(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rohansridhar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running shoe are important for running and runner\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet if you haven't already\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text\n",
    "text = \"running shoes are important for running and runners\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Join the lemmatized words back into a sentence\n",
    "lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "print(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run shoe are import for run and runner\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"running shoes are important for running and runners\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each word\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "# Join the stemmed words back into a sentence\n",
    "stemmed_text = ' '.join(stemmed_words)\n",
    "\n",
    "print(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample text data (you can replace it with your own dataset)\n",
    "texts = [\n",
    "    \"This is a good movie\",\n",
    "    \"The plot was interesting\",\n",
    "    \"I did not like the acting\",\n",
    "    \"The movie was boring\",\n",
    "    \"The acting was excellent\"\n",
    "]\n",
    "# Corresponding labels\n",
    "labels = [1, 1, 0, 0, 1]  # 1 for positive sentiment, 0 for negative sentiment\n",
    "\n",
    "# Step 1: Vectorize the text data using Bag-of-Words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Train a classifier (Logistic Regression in this case)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
