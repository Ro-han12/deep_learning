{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(n):\n",
    "    \"\"\" Initialize weights and bias to zero. \"\"\"\n",
    "    w = np.zeros((n, 1))\n",
    "    b = 0\n",
    "    return w, b\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\" Sigmoid activation function. \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost_and_gradient(w, b, X, Y):\n",
    "    \"\"\" Compute current predictions, cost, and gradient. \"\"\"\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / m\n",
    "    dw = np.dot(X, (A - Y).T) / m\n",
    "    db = np.sum(A - Y) / m\n",
    "    return cost, dw, db\n",
    "\n",
    "def update_parameters(w, b, dw, db, learning_rate):\n",
    "    \"\"\" Update parameters using the gradient. \"\"\"\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "    return w, b\n",
    "\n",
    "def model(X, Y, num_iterations, learning_rate):\n",
    "    \"\"\" Logistic regression model function. \"\"\"\n",
    "    w, b = initialize_parameters(X.shape[0])\n",
    "    for i in range(num_iterations):\n",
    "        cost, dw, db = compute_cost_and_gradient(w, b, X, Y)\n",
    "        w, b = update_parameters(w, b, dw, db, learning_rate)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "    return w, b\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
